{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5240041f",
   "metadata": {},
   "source": [
    "<div style=\"max-width: 90%; background: linear-gradient(90deg, #F7F9FD 0%, #ECF4F8 100%); padding: 25px; border-radius: 10px; text-align: center; margin-bottom: 15px; box-sizing: border-box; border: 2px solid #7AD6EB;\">\n",
    "    <div style=\"margin-bottom: 20px;\">\n",
    "        <div style=\"width: 80px; height: 40px; background: #ECF4F8; border: 0px solid #61BD73; border-radius: 8px; display: inline-flex; align-items: center; justify-content: center; font-size: 0.8em; color: #130C49; font-weight: bold; margin-bottom: 25px;\">\n",
    "            <img src=\"assets/weaviate-logo-light-transparent-200.png\" alt=\"Weaviate Logo\" style=\"height: 30px;\"/>\n",
    "        </div>\n",
    "    </div>\n",
    "    <h1 style=\"font-size: 2.2em; margin: 0 0 25px 0; color: #130C49; font-weight: 700;\">ðŸš€ Building a PDF-Driven RAG System</h1>\n",
    "    <p style=\"font-size: 1.2em; margin: 0 0 20px 0; color: #130C49; opacity: 0.8; font-weight: 400;\">Transform static documents into intelligent, searchable knowledge<br/>with <strong>Weaviate</strong> and multi-modal models</p>\n",
    "    <hr style=\"border: none; height: 1px; background: linear-gradient(90deg, transparent 0%, #61BD73 50%, transparent 100%); margin: 25px 0;\"/>\n",
    "    <p style=\"margin: 0; font-size: 1em; color: #130C49; opacity: 0.9;\">\n",
    "        ðŸ‘€ <strong>Watch along</strong> or ðŸ’» <strong>Code along</strong> - your choice!\n",
    "    </p>    \n",
    "</div>\n",
    "\n",
    "<div style=\"max-width: 90%; background: linear-gradient(90deg, #F7F9FD 0%, #ECF4F8 100%); padding: 15px; border-radius: 10px; text-align: center; margin: 15px 0; box-sizing: border-box; border: 1px solid #B8C0DE;\">\n",
    "    <div style=\"display: flex; align-items: center; justify-content: center; gap: 15px;\">\n",
    "        <div style=\"width: 50px; height: 50px; background: #ECF4F8; border: 2px solid #7AD6EB; border-radius: 50%; display: flex; align-items: center; justify-content: center; font-size: 1.2em; color: #130C49;\">\n",
    "            <img src=\"assets/jp_profile_small_2023.jpeg\" alt=\"JP Hwang\" style=\"width: 46px; height: 46px; border-radius: 50%; object-fit: cover;\"/>\n",
    "        </div>\n",
    "        <div style=\"text-align: left;\">\n",
    "            <h4 style=\"margin: 0; color: #130C49; font-size: 1em;\">JP Hwang</h4>\n",
    "            <p style=\"margin: 2px 0 0 0; color: #130C49; font-size: 0.85em; opacity: 0.8;\">Senior Developer Educator</p>\n",
    "        </div>\n",
    "    </div>\n",
    "</div>\n",
    "\n",
    "<div style=\"max-width: 90%; background: linear-gradient(90deg, #ECF4F8 0%, #F7F9FD 100%); padding: 20px; border-radius: 10px; text-align: center; margin: 15px 0; box-sizing: border-box; border: 2px solid #61BD73;\">\n",
    "    <div style=\"max-width: 90%; background: rgba(97,189,115,0.1); padding: 15px; border-radius: 6px;\">\n",
    "        <h3 style=\"margin: 0 0 10px 0; color: #130C49; font-size: 1.1em;\">ðŸ”§ Setup Required (Only if coding along)</h3>\n",
    "        <p style=\"color: #130C49; font-size: 0.9em; margin-bottom: 10px;\">\n",
    "            Follow the <strong>setup instructions</strong> in our README<br/><br/><a href=\"https://github.com/weaviate-tutorials/workshop-pdf-driven-rag/\" style=\"color: #130C49; text-decoration: underline;\">https://github.com/weaviate-tutorials/workshop-pdf-driven-rag/</a>\n",
    "        </p>\n",
    "        <div style=\"text-align: left; display: inline-block;\">\n",
    "            <p style=\"color: #130C49; font-weight: bold; margin: 5px 0; font-size: 0.9em;\">\n",
    "            âœ… Python environment configured<br>\n",
    "            âœ… Required dependencies installed<br>\n",
    "            âœ… API keys set up in <code style=\"background: #130C49; color: white; padding: 2px 4px; border-radius: 3px;\">.env</code> file<br>\n",
    "            </p>\n",
    "        </div>\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3db88ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocess_pdf_to_img import convert_pdf_to_images\n",
    "from pathlib import Path\n",
    "\n",
    "img_dir = Path(\"data/imgs\")\n",
    "for pdf_path in [\n",
    "    Path(\"data/pdfs/hai_ai_index_report_2025_chapter_2.pdf\"),\n",
    "    Path(\"data/pdfs/howto-free-threading-python.pdf\")\n",
    "]:\n",
    "    convert_pdf_to_images(pdf_path, img_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d83e50b",
   "metadata": {},
   "source": [
    "![assets/about_weaviate.png](assets/about_weaviate.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1150d8c8",
   "metadata": {},
   "source": [
    "![assets/workshop_agenda.jpg](assets/workshop_agenda.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d06d56d3",
   "metadata": {},
   "source": [
    "## Preparation\n",
    "\n",
    "Load environment variables. If you haven't set up your `.env` file, please refer to the [README](README.md).\n",
    "\n",
    "In the live session, your instructor may provide you with temporary keys to use. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "84232a36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "556603ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sk-ant-api\n",
      "RRSFuhizdT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "print(os.getenv(\"ANTHROPIC_API_KEY\")[:10])\n",
    "print(os.getenv(\"COHERE_API_KEY\")[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16de5370",
   "metadata": {},
   "source": [
    "## Working with PDFs - an introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b8693ef",
   "metadata": {},
   "source": [
    "### Get text from PDFs\n",
    "\n",
    "PDFs contain rich formatting:\n",
    "\n",
    "<img src=\"data/imgs/howto-free-threading-python_2_of_4.png\" height=\"400px\" />\n",
    "<img src=\"data/imgs/hai_ai_index_report_2025_chapter_2_34_of_80.jpg\" height=\"400px\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de250ab",
   "metadata": {},
   "source": [
    "Modern libraries can preserve document structure while converting to text format, making the text easier to process while maintaining semantic relationships."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9010e68",
   "metadata": {},
   "source": [
    "Let's try a popular library, `docling`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d94c728a",
   "metadata": {},
   "source": [
    "Here's how we can convert PDFs into text files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e9a44a12-40dc-4e92-af6e-0a65d2e11d8b",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": null,
    "lastExecutedAt": null,
    "lastExecutedByKernel": null,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": null,
    "outputsMetadata": {
     "0": {
      "height": 437,
      "type": "stream"
     },
     "1": {
      "height": 143,
      "type": "stream"
     },
     "2": {
      "height": 80,
      "type": "stream"
     }
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from docling.document_converter import DocumentConverter\n",
    "\n",
    "\n",
    "def parse_pdf(input_doc_path: Path, output_dir: Path):\n",
    "    doc_converter = DocumentConverter()\n",
    "    conv_res = doc_converter.convert(input_doc_path)\n",
    "\n",
    "    # Save markdown\n",
    "    filename = input_doc_path.stem\n",
    "    md_filepath = output_dir / f\"{filename}-parsed-text.md\"\n",
    "    if not md_filepath.exists():\n",
    "        output_dir.mkdir(parents=True, exist_ok=True)\n",
    "        print(f\"Saving parsed text to {md_filepath}\")\n",
    "        with md_filepath.open(\"w\", encoding=\"utf-8\") as md_file:\n",
    "            md_file.write(conv_res.document.export_to_markdown())\n",
    "    else:\n",
    "        print(f\"Parsed text already exists at {md_filepath}, skipping text extraction.\")\n",
    "    return md_filepath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3a55c215",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = Path(\"data/pdfs\")\n",
    "output_dir = Path(\"data/parsed\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "input_pdf_name = \"howto-free-threading-python.pdf\"\n",
    "input_doc_path = data_folder / input_pdf_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "98f604b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed text already exists at data/parsed/howto-free-threading-python-parsed-text.md, skipping text extraction.\n"
     ]
    }
   ],
   "source": [
    "# Run the parser function\n",
    "# BEGIN_SOLUTION\n",
    "md_filepath = parse_pdf(input_doc_path, output_dir)\n",
    "# END_SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e8bd52f",
   "metadata": {},
   "source": [
    "Inspect the converted file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ad6cf454",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Python experimental support for free threading\n",
      "\n",
      "Release 3.13.3\n",
      "\n",
      "## Guido van Rossum and the Python development team\n",
      "\n",
      "April 27, 2025\n",
      "\n",
      "Python Software Foundation Email: docs@python.org\n",
      "\n",
      "## Contents\n",
      "\n",
      "| 1     | Installation                                        |   2 |\n",
      "|-------|-----------------------------------------------------|-----|\n",
      "| 2     | Identifying free-threaded Python                    |   2 |\n",
      "| 3     | The global interpreter lock in free-threaded Python |   2 |\n",
      "| 4     | Thread safety                                       |   2 |\n",
      "| 5     | Known limitations                                   |   2 |\n",
      "| 5.1   | Immortalization . . . . . . . . . . . . . . . . .   |   3 |\n",
      "| 5.2   | Frame objects . . . . . . . . . . . . . . . . . .   |   3 |\n",
      "| 5.3   | Iterators . . . . . . . . . . . . . . . . . . . . . |   3 |\n",
      "| 5.4   | Single-threaded performance . . . . . . . . . .     |   3 |\n",
      "| Index |                                                     |   4 |\n",
      "\n",
      "Starting with the 3.13 releas\n"
     ]
    }
   ],
   "source": [
    "md_txt = md_filepath.read_text()\n",
    "\n",
    "# Print some part of the parsed text:\n",
    "# BEGIN_SOLUTION\n",
    "print(md_txt[:1000])\n",
    "# END_SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b62b17",
   "metadata": {},
   "source": [
    "### Chunking\n",
    "\n",
    "Chunking breaks documents into smaller, manageable pieces while preserving context. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a50681c2",
   "metadata": {},
   "source": [
    "![assets/chunking_why.png](assets/chunking_why.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05bddf09",
   "metadata": {},
   "source": [
    "#### RAG - Recap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a848949",
   "metadata": {},
   "source": [
    "![assets/llm_2_rag_basic.png](assets/llm_2_rag_basic.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc66bb32",
   "metadata": {},
   "source": [
    "What does this have to do with chunking? \n",
    "\n",
    "Each \"chunk\" becomes a \"unit\" of context to feed into a RAG input!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd8ec2ff",
   "metadata": {},
   "source": [
    "### Chunking strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99973c13",
   "metadata": {},
   "source": [
    "Different chunking strategies serve different use cases. \n",
    "\n",
    "![assets/chunking_methods.png](assets/chunking_methods.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da9c3c26",
   "metadata": {},
   "source": [
    "Let's try a few options:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ddfa414",
   "metadata": {},
   "source": [
    "#### Chunk by text length with overlap\n",
    "\n",
    "Overlapping chunks help maintain context across boundaries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ad3329d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chunks_by_length_with_overlap(src_text: str, chunk_length: int = 500, overlap: int = 100) -> list[str]:\n",
    "    \"\"\"\n",
    "    Split text into chunks of approximately `chunk_length` characters.\n",
    "    \"\"\"\n",
    "    chunks = []\n",
    "    for i in range(0, len(src_text), chunk_length):\n",
    "        chunks.append(src_text[i:i + chunk_length + overlap])\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dbc39a6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['## Python experimental support for free threading\\n\\nRelease 3.13.3\\n\\n## Guido van Rossum and the Python development team\\n\\nApril 27, 2025\\n\\nPython Software Foundation Email: docs@python.org\\n\\n## Contents\\n\\n| 1     | Installation                                        |   2 |\\n|-------|-----------------------------------------------------|-----|\\n| 2     | Identifying free-threaded Python                    |   2 |\\n| 3     | The global interpreter lock in free-threaded Python |   2 |\\n| 4     | Thread safety                                       |   2 |\\n| 5     | Known limitations                       ',\n",
       " 'ety                                       |   2 |\\n| 5     | Known limitations                                   |   2 |\\n| 5.1   | Immortalization . . . . . . . . . . . . . . . . .   |   3 |\\n| 5.2   | Frame objects . . . . . . . . . . . . . . . . . .   |   3 |\\n| 5.3   | Iterators . . . . . . . . . . . . . . . . . . . . . |   3 |\\n| 5.4   | Single-threaded performance . . . . . . . . . .     |   3 |\\n| Index |                                                     |   4 |\\n\\nStarting with the 3.13 release, CPython has experimental support for a build of Python called free threading where the global int',\n",
       " 'e, CPython has experimental support for a build of Python called free threading where the global interpreter lock (GIL) is disabled. Free-threaded execution allows for full utilization of the available processing power by running threads in parallel on available CPU cores. While not all software will benefit from this automatically, programs designed with threading in mind will run faster on multi-core hardware.\\n\\nThe free-threaded mode is experimental and work is ongoing to improve it: expect some bugs and a substantial single-threaded performance hit.\\n\\nThis document describes the implications',\n",
       " 'me bugs and a substantial single-threaded performance hit.\\n\\nThis document describes the implications of free threading for Python code. See freethreading-extensions-howto for information on how to write C extensions that support the free-threaded build.\\n\\n<!-- image -->\\n\\n| Âµ See also                                                                                                           |\\n|----------------------------------------------------------------------------------------------------------------------|\\n| PEP 703 - Making the Global Interpreter Lock Optional in CPython for an overall descr',\n",
       " '-----------|\\n| PEP 703 - Making the Global Interpreter Lock Optional in CPython for an overall description of free-threaded Python. |\\n\\n## 1 Installation\\n\\nStarting with Python 3.13, the official macOS and Windows installers optionally support installing free-threaded Python binaries. The installers are available at https://www.python.org/downloads/.\\n\\nFor information on other platforms, see the Installing a Free-Threaded Python, a community-maintained installation guide for installing free-threaded Python.\\n\\nWhen building CPython from source, the --disable-gil configure option should be used to b']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "600\n"
     ]
    }
   ],
   "source": [
    "# Try chunking using the get_chunks_by_length_with_overlap method\n",
    "# BEGIN_SOLUTION\n",
    "chunks = get_chunks_by_length_with_overlap(md_txt)\n",
    "# END_SOLUTION\n",
    "\n",
    "# Have a look at some of the chunks\n",
    "display(chunks[:5])\n",
    "print(len(chunks[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b694e768",
   "metadata": {},
   "source": [
    "#### Chunk using markers\n",
    "\n",
    "Using document markers (like headers) creates chunks that respect natural document boundaries. \n",
    "\n",
    "This approach preserves semantic structure and is ideal for documents with clear hierarchical organization like reports, manuals, or academic papers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "35daea7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chunks_using_markers(src_text: str) -> list[str]:\n",
    "    \"\"\"\n",
    "    Split the source text into chunks using markers.\n",
    "    \"\"\"\n",
    "    marker = \"\\n##\"\n",
    "\n",
    "    # Split by marker and reconstruct with markers (except first chunk)\n",
    "    parts = src_text.split(marker)\n",
    "    chunks = []\n",
    "\n",
    "    # Add first chunk if it exists and isn't empty\n",
    "    if parts[0].strip():\n",
    "        chunks.append(parts[0].strip())\n",
    "\n",
    "    # Add remaining chunks with markers reattached\n",
    "    for part in parts[1:]:\n",
    "        if part.strip():\n",
    "            chunks.append(marker + part.strip())\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ccd11080",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['## Python experimental support for free threading\\n\\nRelease 3.13.3',\n",
       " '\\n##Guido van Rossum and the Python development team\\n\\nApril 27, 2025\\n\\nPython Software Foundation Email: docs@python.org',\n",
       " '\\n##Contents\\n\\n| 1     | Installation                                        |   2 |\\n|-------|-----------------------------------------------------|-----|\\n| 2     | Identifying free-threaded Python                    |   2 |\\n| 3     | The global interpreter lock in free-threaded Python |   2 |\\n| 4     | Thread safety                                       |   2 |\\n| 5     | Known limitations                                   |   2 |\\n| 5.1   | Immortalization . . . . . . . . . . . . . . . . .   |   3 |\\n| 5.2   | Frame objects . . . . . . . . . . . . . . . . . .   |   3 |\\n| 5.3   | Iterators . . . . . . . . . . . . . . . . . . . . . |   3 |\\n| 5.4   | Single-threaded performance . . . . . . . . . .     |   3 |\\n| Index |                                                     |   4 |\\n\\nStarting with the 3.13 release, CPython has experimental support for a build of Python called free threading where the global interpreter lock (GIL) is disabled. Free-threaded execution allows for full utilization of the available processing power by running threads in parallel on available CPU cores. While not all software will benefit from this automatically, programs designed with threading in mind will run faster on multi-core hardware.\\n\\nThe free-threaded mode is experimental and work is ongoing to improve it: expect some bugs and a substantial single-threaded performance hit.\\n\\nThis document describes the implications of free threading for Python code. See freethreading-extensions-howto for information on how to write C extensions that support the free-threaded build.\\n\\n<!-- image -->\\n\\n| Âµ See also                                                                                                           |\\n|----------------------------------------------------------------------------------------------------------------------|\\n| PEP 703 - Making the Global Interpreter Lock Optional in CPython for an overall description of free-threaded Python. |',\n",
       " '\\n##1 Installation\\n\\nStarting with Python 3.13, the official macOS and Windows installers optionally support installing free-threaded Python binaries. The installers are available at https://www.python.org/downloads/.\\n\\nFor information on other platforms, see the Installing a Free-Threaded Python, a community-maintained installation guide for installing free-threaded Python.\\n\\nWhen building CPython from source, the --disable-gil configure option should be used to build a free-threaded Python interpreter.',\n",
       " '\\n##2 Identifying free-threaded Python\\n\\nTo check if the current interpreter supports free-threading, python -VV and sys.version contain \\'experimental free-threading build\\'. The new sys.\\\\_is\\\\_gil\\\\_enabled() function can be used to check whether the GIL is actually disabled in the running process.\\n\\nThe sysconfig.get\\\\_config\\\\_var(\"Py\\\\_GIL\\\\_DISABLED\") configuration variable can be used to determine whether the build supports free threading. If the variable is set to 1 , then the build supports free threading. This is the recommended mechanism for decisions related to the build configuration.']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65\n"
     ]
    }
   ],
   "source": [
    "# Try chunking using the marker-based method\n",
    "# BEGIN_SOLUTION\n",
    "chunks = get_chunks_using_markers(md_txt)\n",
    "# END_SOLUTION\n",
    "\n",
    "# Have a look at some of the chunks\n",
    "display(chunks[:5])\n",
    "print(len(chunks[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af0841b2",
   "metadata": {},
   "source": [
    "### Choosing the right strategy\n",
    "\n",
    "The best chunking strategy depends on your use case. \n",
    "\n",
    "Marker-based chunking excels with structured documents - but in some cases, it may not work as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4251fc27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['## Arti fi cial Intelligence Index Report 2025\\n\\n<!-- image -->\\n\\n<!-- image -->\\n\\n<!-- image -->',\n",
       " '\\n##Chapter 2: Technical Performance\\n\\nOverview\\n\\n84\\n\\nChapter Highlights\\n\\n85',\n",
       " '\\n##2.1 Overview of AI in 2024\\n\\n87\\n\\nTimeline: Signi fi cant Model and Dataset Releases\\n\\n87\\n\\nState of AI Performance\\n\\n93\\n\\nOverall Review\\n\\n93\\n\\nClosed vs. Open-Weight Models\\n\\n94\\n\\nUS vs. China Technical Performance\\n\\n96\\n\\nImproved Performance From Smaller Models\\n\\n98\\n\\nModel Performance Converges at the Frontier\\n\\n99\\n\\nBenchmarking AI\\n\\n100',\n",
       " '\\n##2.2 Language',\n",
       " '\\n##103\\n\\n| Understanding                                        |   104 |\\n|------------------------------------------------------|-------|\\n| MMLU: Massive Multitask Language Understanding       |   104 |\\n| Generation                                           |   105 |\\n| Chatbot Arena Leaderboard                            |   105 |\\n| Arena-Hard-Auto                                      |   107 |\\n| WildBench                                            |   108 |\\n| Highlight: o1, o3, and Inference- Time Compute       |   110 |\\n| MixEval                                              |   112 |\\n| RAG: Retrieval Augment Generation                    |   113 |\\n| Berkeley Function Calling Leaderboard                |   113 |\\n| MTEB: Massive Text Embedding Benchmark               |   115 |\\n| Highlight: Evaluating Retrieval Across Long Contexts |   117 |\\n| 2.3 Image and Video                                  |   119 |\\n| Understanding                                        |   119 |\\n\\nVCR: Visual Commonsense Reasoning 119\\n\\nMVBench\\n\\n120\\n\\nGeneration\\n\\n122\\n\\nChatbot Arena: Vision\\n\\n123\\n\\nHighlight: The Rise of Video Generation\\n\\n124',\n",
       " '\\n##2.4 Speech\\n\\n126\\n\\nSpeech Recognition\\n\\n126\\n\\nLSR2: Lip Reading Sentences 2\\n\\n126',\n",
       " '\\n##2.5 Coding\\n\\n128\\n\\n| HumanEval             |   128 |\\n|-----------------------|-------|\\n| SWE-bench             |   129 |\\n| BigCodeBench          |   130 |\\n| Chatbot Arena: Coding |   131 |',\n",
       " '\\n##2.6 Mathematics\\n\\n132\\n\\n| GSM8K                                   |   132 |\\n|-----------------------------------------|-------|\\n| MATH                                    |   133 |\\n| Chatbot Arena: Math                     |   134 |\\n| FrontierMath                            |   134 |\\n| Highlight: Learning and Theorem Proving |   136 |',\n",
       " \"\\n##2.7 Reasoning\\n\\n137\\n\\nGeneral Reasoning\\n\\n137\\n\\nMMMU: A Massive Multi-discipline Multimodal Understanding and Reasoning Benchmark for Expert AGI\\n\\n137\\n\\nGPQA: A Graduate-Level Google-Proof Q&amp;A Benchmark\\n\\n138\\n\\nARC-AGI\\n\\n139\\n\\nHumanity's Last Exam\\n\\n141\\n\\nPlanning\\n\\n143\\n\\nPlanBench\\n\\n143\\n\\n<!-- image -->\\n\\n<!-- image -->\",\n",
       " \"\\n##Chapter 2: Technical Performance (cont'd)\\n\\n| 2.8 AI Agents    |   144 |\\n|------------------|-------|\\n| VisualAgentBench |   144 |\\n| RE-Bench         |   145 |\\n| GAIA             |   147 |\\n\\n| 2.9 Robotics and Autonomous Motion        |   148 |\\n|-------------------------------------------|-------|\\n| Robotics                                  |   148 |\\n| RLBench                                   |   148 |\\n| Highlight: Humanoid Robotics              |   150 |\\n| Highlight: DeepMind's Developments        |   151 |\\n| Highlight: Foundation Models for Robotics |   154 |\\n| Self-Driving Cars                         |   155 |\\n| Deployment                                |   155 |\\n| Technical Innovations and New Benchmarks  |   156 |\\n| Safety Standards                          |   157 |\",\n",
       " '\\n##ACCESS THE PUBLIC DATA',\n",
       " '\\n##CHAPTER 2: Technical Performance',\n",
       " \"\\n##Overview\\n\\nThe Technical Performance section of this year's AI Index provides a comprehensive overview  of  AI  advancements  in  2024.  It  begins  with  a  high-level  summary  of  AI technical progress, covering major AI-related launches, the state of AI capabilities, and key trends-such as the rising performance of open-weight models, the convergence of  frontier  model  performance,  and  the  improving  quality  of  Chinese  LLMs.  The chapter then examines the current state of various AI capabilities, including language understanding and generation, retrieval-augmented generation, coding, mathematics, reasoning, computer vision, speech, and agentic AI. New this year are signi fi cantly expanded analyses of performance trends in robotics and self-driving cars.\\n\\n<!-- image -->\\n\\n<!-- image -->\",\n",
       " \"\\n##Chapter Highlights\\n\\n1.  AI  masters  new  benchmarks  faster  than  ever. In  2023,  AI  researchers  introduced  several  challenging  new benchmarks, including MMMU, GPQA, and SWE-bench, aimed at testing the limits of increasingly capable AI systems. By 2024, AI performance on these benchmarks saw remarkable improvements, with gains of 18.8 and 48.9 percentage points on MMMU and GPQA, respectively. On SWE-bench, AI systems could solve just 4.4% of coding problems in 2023-a fi gure that jumped to 71.7% in 2024.\\n2. Open-weight models catch up. Last year's AI Index revealed that leading open-weight models lagged signi fi cantly behind their closed-weight counterparts. By 2024, this gap had nearly disappeared. In early January 2024, the leading closedweight model outperformed the top open-weight model by 8.04% on the Chatbot Arena Leaderboard. By February 2025, this gap had narrowed to 1.70%.\\n3. The gap between Chinese and US models closes. In 2023, leading American models signi fi cantly outperformed their Chinese counterparts-a trend that no longer holds. At the end of 2023, performance gaps on benchmarks such as MMLU, MMMU, MATH, and HumanEval were 17.5, 13.5, 24.3, and 31.6 percentage points, respectively. By the end of 2024, these di ff erences had narrowed substantially to just 0.3, 8.1, 1.6, and 3.7 percentage points.\\n4. AI model performance converges at the frontier. According to last year's AI Index, the Elo score di ff erence between the top and 10th-ranked model on the Chatbot Arena Leaderboard was 11.9%. By early 2025, this gap had narrowed to just 5.4%. Likewise, the di ff erence between the top two models shrank from 4.9% in 2023 to just 0.7% in 2024. The AI landscape is becoming increasingly competitive, with high-quality models now available from a growing number of developers.\\n5. New reasoning paradigms like test-time compute improve model performance. In 2024, OpenAI introduced models like o1 and o3 that are designed to iteratively reason through their outputs. This test-time compute approach dramatically improved performance, with o1 scoring 74.4% on an International Mathematical Olympiad qualifying exam, compared to GPT4o's 9.3%. However, this enhanced reasoning comes at a cost: o1 is nearly six times more expensive and 30 times slower than GPT-4o.\\n\\n<!-- image -->\\n\\n<!-- image -->\",\n",
       " \"\\n##Chapter Highlights (cont'd)\\n\\n6. More challenging benchmarks are continually proposed. The saturation of traditional AI benchmarks like MMLU, GSM8K, and HumanEval, coupled with improved performance on newer, more challenging benchmarks such as MMMU and GPQA, has pushed researchers to explore additional evaluation methods for leading AI systems. Notable among these are Humanity's Last Exam, a rigorous academic test where the top system scores just 8.80%; FrontierMath, a complex mathematics benchmark where AI systems solve only 2% of problems; and BigCodeBench, a coding benchmark where AI systems achieve a 35.5% success rate-well below the human standard of 97%.\\n\\n7. High-quality AI video generators demonstrate signi fi cant improvement. In 2024, several advanced AI models capable of generating high-quality videos from text inputs were launched. Notable releases include OpenAI's SORA, Stable Video 3D and 4D, Meta's Movie Gen, and Google DeepMind's Veo 2. These models produce videos of signi fi cantly higher quality compared to those from 2023.\\n\\n8. Smaller models drive stronger performance. In 2022, the smallest model registering a score higher than 60% on MMLU was PaLM, with 540 billion parameters. By 2024, Microsoft's Phi-3-mini, with just 3.8 billion parameters, achieved the same threshold. This represents a 142-fold reduction in over two years.\\n9.  Complex  reasoning  remains  a  problem. Even  though  the  addition  of  mechanisms  such  as  chain-of-thought reasoning has signi fi cantly improved the performance of LLMs, these systems still cannot reliably solve problems for which provably correct solutions can be found using logical reasoning, such as arithmetic and planning, especially on instances larger than those they were trained on. This has a signi fi cant impact on the trustworthiness of these systems and their suitability in high-risk applications.\\n10. AI agents show early promise. The launch of RE-Bench in 2024 introduced a rigorous benchmark for evaluating complex tasks for AI agents. In short time-horizon settings (two-hour budget), top AI systems score four times higher than human experts, but as the time budget increases, human performance surpasses AI-outscoring it two to one at 32 hours. AI agents already match human expertise in select tasks, such as writing Triton kernels, while delivering results faster and at lower costs.\\n\\n<!-- image -->\"]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "md_file_2 = Path(\"data/parsed/hai_ai_index_report_2025_chapter_2-parsed-text.md\")\n",
    "md_text_2 = md_file_2.read_text(encoding=\"utf-8\")\n",
    "get_chunks_using_markers(md_text_2)[:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96380962",
   "metadata": {},
   "source": [
    "Here, the page headers are mistakenly interpreted as headings, which confuses our structure. \n",
    "\n",
    "<img src=\"data/imgs/hai_ai_index_report_2025_chapter_2_05_of_80.jpg\" height=\"400px\" />\n",
    "<img src=\"data/imgs/hai_ai_index_report_2025_chapter_2_06_of_80.jpg\" height=\"400px\" />\n",
    "\n",
    "As a result, a length-based chunking method tends to be quite robust."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2343c7fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "editor": "DataLab",
  "kernelspec": {
   "display_name": "workshop-pdf-rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
