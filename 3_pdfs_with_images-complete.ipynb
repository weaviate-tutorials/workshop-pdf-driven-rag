{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8d06ab9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c4a081d",
   "metadata": {},
   "source": [
    "## Working with PDFs with images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "953ce690",
   "metadata": {},
   "source": [
    "PDFs contain more than rich formatting - they have images!\n",
    "\n",
    "<img src=\"data/imgs/hai_ai_index_report_2025_chapter_2_34_of_80.jpg\" width=\"200px\" />\n",
    "<img src=\"data/imgs/hai_ai_index_report_2025_chapter_2_58_of_80.jpg\" width=\"200px\" />\n",
    "<img src=\"data/imgs/hai_ai_index_report_2025_chapter_2_69_of_80.jpg\" width=\"200px\" />\n",
    "\n",
    "How do we work with these for RAG?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d2d4de",
   "metadata": {},
   "source": [
    "### Approach 1 - Extract text and images separately"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c0287c",
   "metadata": {},
   "source": [
    "Some libraries (like `docling`) can extract text and images from PDFs, and convert them into Markdown files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e3ea399d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "data_folder = Path(\"data/pdfs\")\n",
    "output_dir = Path(\"data/parsed\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "23684011",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: howto-free-threading-python.pdf\n",
      "Converting document data/pdfs/howto-free-threading-python.pdf to multimodal pages...\n",
      "Skipping data/parsed/howto-free-threading-python-parsed-w-imgs.md as it already exists.\n",
      "Processing file: manual_bosch_WGG254Z0GR.pdf\n",
      "Converting document data/pdfs/manual_bosch_WGG254Z0GR.pdf to multimodal pages...\n",
      "Skipping data/parsed/manual_bosch_WGG254Z0GR-parsed-w-imgs.md as it already exists.\n",
      "Processing file: hai_ai_index_report_2025_chapter_2.pdf\n",
      "Converting document data/pdfs/hai_ai_index_report_2025_chapter_2.pdf to multimodal pages...\n",
      "Skipping data/parsed/hai_ai_index_report_2025_chapter_2-parsed-w-imgs.md as it already exists.\n"
     ]
    }
   ],
   "source": [
    "from docling.datamodel.base_models import InputFormat\n",
    "from docling.datamodel.pipeline_options import PdfPipelineOptions\n",
    "from docling.document_converter import DocumentConverter, PdfFormatOption\n",
    "from docling_core.types.doc import ImageRefMode\n",
    "\n",
    "IMAGE_RESOLUTION_SCALE = 2.0\n",
    "\n",
    "\n",
    "def parse_pdf_with_images(input_doc_path: Path, output_dir: Path):\n",
    "    # Reference: https://docling-project.github.io/docling/examples/export_figures/\n",
    "    md_filename = output_dir / f\"{input_doc_path.name.split('.')[0]}-parsed-w-imgs.md\"\n",
    "    if md_filename.exists():\n",
    "        print(f\"Skipping {md_filename} as it already exists.\")\n",
    "        return\n",
    "\n",
    "    pipeline_options = PdfPipelineOptions()\n",
    "    pipeline_options.images_scale = IMAGE_RESOLUTION_SCALE\n",
    "    pipeline_options.generate_picture_images = True\n",
    "\n",
    "    doc_converter = DocumentConverter(\n",
    "        format_options={\n",
    "            InputFormat.PDF: PdfFormatOption(pipeline_options=pipeline_options)\n",
    "        }\n",
    "    )\n",
    "\n",
    "    conv_res = doc_converter.convert(input_doc_path)\n",
    "\n",
    "    # Save markdown with embedded pictures\n",
    "    conv_res.document.save_as_markdown(md_filename, image_mode=ImageRefMode.REFERENCED)\n",
    "\n",
    "\n",
    "pdf_names = [f.name for f in data_folder.glob(\"*.pdf\") if f.is_file()]\n",
    "\n",
    "for pdf_fname in pdf_names:\n",
    "    print(f\"Processing file: {pdf_fname}\")\n",
    "\n",
    "    input_doc_path = data_folder / pdf_fname\n",
    "\n",
    "    print(f\"Converting document {input_doc_path} to multimodal pages...\")\n",
    "    parse_pdf_with_images(input_doc_path, output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "22e8a60c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Arti fi cial Intelligence Index Report 2025\n",
      "\n",
      "![Image](hai_ai_index_report_2025_chapter_2-parsed-w-imgs_artifacts/image_000000_c6ec2f8165962bd018633d07074eaf41cfb78512dd53037fa2fcdda5ff3e6b52.png)\n",
      "\n",
      "![Image](hai_ai_index_report_2025_chapter_2-parsed-w-imgs_artifacts/image_000001_23acef5ec9d7e4fa2bc47b734cc808da8578bc8d91748aea27d76aedc4f31dd0.png)\n",
      "\n",
      "![Image](hai_ai_index_report_2025_chapter_2-parsed-w-imgs_artifacts/image_000002_2ef37eac1d46d33cf738050ed266e05091775c5ea224676eb6a41aeb01cb00f1.png)\n",
      "\n",
      "## Chapter 2: Technical Performance\n",
      "\n",
      "Overview\n",
      "\n",
      "84\n",
      "\n",
      "Chapter Highlights\n",
      "\n",
      "85\n",
      "\n",
      "## 2.1 Overview of AI in 2024\n",
      "\n",
      "87\n",
      "\n",
      "Timeline: Signi fi cant Model and Dataset Releases\n",
      "\n",
      "87\n",
      "\n",
      "State of AI Performance\n",
      "\n",
      "93\n",
      "\n",
      "Overall Review\n",
      "\n",
      "93\n",
      "\n",
      "Closed vs. Open-Weight Models\n",
      "\n",
      "94\n",
      "\n",
      "US vs. China Technical Performance\n",
      "\n",
      "96\n",
      "\n",
      "Improved Performance From Smaller Models\n",
      "\n",
      "98\n",
      "\n",
      "Model Performance Converges at the Frontier\n",
      "\n",
      "99\n",
      "\n",
      "Benchmarking AI\n",
      "\n",
      "100\n",
      "\n",
      "## 2.2 Language\n",
      "\n",
      "## 103\n",
      "\n",
      "| Understanding                                        |   104 |\n"
     ]
    }
   ],
   "source": [
    "md_filepath = Path(\"data/parsed/hai_ai_index_report_2025_chapter_2-parsed-w-imgs.md\")\n",
    "md_txt = md_filepath.read_text()\n",
    "print(md_txt[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f0297db",
   "metadata": {},
   "source": [
    "#### Chunking text files with images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ee9281",
   "metadata": {},
   "source": [
    "More complex than just text, since we need to handle images as well.\n",
    "\n",
    "- Must include entire image string in the chunk\n",
    "- When vectorizing, optionally include base64 of image\n",
    "    - Your embedding model must be multimodal\n",
    "\n",
    "Chunking becomes more complex."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd48bb25",
   "metadata": {},
   "source": [
    "One method: try a specialized library like `chonkie` to handle this\n",
    "\n",
    "Chonkie offers a variety of chunking strategies:\n",
    "\n",
    "<img src=\"assets/chonkie_methods.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aeea62c",
   "metadata": {},
   "source": [
    "There isn't going to be a \"one size fits all\" solution for chunking PDFs with images. But these libraries can help you get started."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9daef5ae",
   "metadata": {},
   "source": [
    "Let's try a couple of different approaches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7adb5c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "from chonkie import RecursiveChunker\n",
    "\n",
    "# Initialize the recursive chunker to chunk Markdown\n",
    "chunker = RecursiveChunker.from_recipe(\"markdown\", lang=\"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7d61fca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_texts = chunker.chunk(md_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4d8d48b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================\n",
      "Token count: 911\n",
      "Chunk text:\n",
      "    ## Arti fi cial Intelligence Index Report 2025\n",
      "    ![Image](hai_ai_index_report_2025_chapter_2-parsed-w-imgs_artifacts/image_000000\n",
      "    _c6ec2f8165962bd018633d07074eaf41cfb78512dd53037fa2fcdda5ff3e6b52.png)\n",
      "    ![Image](hai_ai_index_report_2025_chapter_2-parsed-w-imgs_artifacts/image_000001\n",
      "    _23acef5ec9d7e4fa2bc47b734cc808da8578bc8d91748aea27d76aedc4f31dd0.png)\n",
      "    ![Image](hai_ai_index_report_2025_chapter_2-parsed-w-imgs_artifacts/image_000002\n",
      "    _2ef37eac1d46d33cf738050ed266e05091775c5ea224676eb6a41aeb01cb00f1.pn...\n",
      "\n",
      "========================================\n",
      "Token count: 1755\n",
      "Chunk text:\n",
      "    ## 2.2 Language  ## 103  | Understanding\n",
      "    |   104 | |------------------------------------------------------|-------| |\n",
      "    MMLU: Massive Multitask Language Understanding       |   104 | | Generation\n",
      "    |   105 | | Chatbot Arena Leaderboard                            |   105 | |\n",
      "    Arena-Hard-Auto                                      |   107 | | WildBench\n",
      "    |   108 | | Highlight: o1, o3,...\n",
      "\n",
      "========================================\n",
      "Token count: 1437\n",
      "Chunk text:\n",
      "    ## 2.7 Reasoning  137  General Reasoning  137  MMMU: A Massive Multi-discipline\n",
      "    Multimodal Understanding and Reasoning Benchmark for Expert AGI  137  GPQA: A\n",
      "    Graduate-Level Google-Proof Q&amp;A Benchmark  138  ARC-AGI  139  Humanity's\n",
      "    Last Exam  141  Planning  143  PlanBench  143\n",
      "    ![Image](hai_ai_index_report_2025_chapter_2-parsed-w-imgs_artifacts/image_000003\n",
      "    _4ad1321e55f50d6f7d0842da3bf083125f2096368e2d323ba074c40c9428ee29.png)\n",
      "    ![Image](hai_ai_index_report_2025_chapter_2-parsed-w-imgs_artifact...\n",
      "\n",
      "========================================\n",
      "Token count: 1084\n",
      "Chunk text:\n",
      "    ## Overview  The Technical Performance section of this year's AI Index provides\n",
      "    a comprehensive overview  of  AI  advancements  in  2024.  It  begins  with  a\n",
      "    high-level  summary  of  AI technical progress, covering major AI-related\n",
      "    launches, the state of AI capabilities, and key trends-such as the rising\n",
      "    performance of open-weight models, the convergence of  frontier  model\n",
      "    performance,  and  the  improving  quality  of  Chinese  LLMs.  The chapter then\n",
      "    examines the current state of various A...\n",
      "\n",
      "========================================\n",
      "Token count: 1826\n",
      "Chunk text:\n",
      "    ## Chapter Highlights  1.  AI  masters  new  benchmarks  faster  than  ever. In\n",
      "    2023,  AI  researchers  introduced  several  challenging  new benchmarks,\n",
      "    including MMMU, GPQA, and SWE-bench, aimed at testing the limits of increasingly\n",
      "    capable AI systems. By 2024, AI performance on these benchmarks saw remarkable\n",
      "    improvements, with gains of 18.8 and 48.9 percentage points on MMMU and GPQA,\n",
      "    respectively. On SWE-bench, AI systems could solve just 4.4% of coding problems\n",
      "    in 2023-a fi gure that jump...\n"
     ]
    }
   ],
   "source": [
    "import textwrap\n",
    "\n",
    "for chunk in chunk_texts[:5]:\n",
    "    print(f\"\\n\" + \"=\" * 40)\n",
    "    print(f\"Token count: {chunk.token_count}\")\n",
    "    print(f\"Chunk text:\")\n",
    "    wrapped_text = textwrap.fill(chunk.text[:500]+\"...\", width=80)\n",
    "    print(textwrap.indent(wrapped_text, \"    \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "991dde5e",
   "metadata": {},
   "source": [
    "Let's try a \"semantic\" chunker:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bc60d639",
   "metadata": {},
   "outputs": [],
   "source": [
    "from chonkie import SemanticChunker\n",
    "\n",
    "# Basic initialization with default parameters\n",
    "chunker = SemanticChunker(\n",
    "    embedding_model=\"minishlab/potion-base-8M\",  # Default model\n",
    "    threshold=0.5,                               # Similarity threshold (0-1) or (1-100) or \"auto\"\n",
    "    chunk_size=2048,                              # Maximum tokens per chunk\n",
    "    min_sentences=1                              # Initial sentences per chunk\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "29aed757",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunk text into `chunk_texts` as we've done before\n",
    "# BEGIN_SOLUTION\n",
    "chunk_texts = chunker.chunk(md_txt)\n",
    "# END_SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e7042aec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================\n",
      "Token count: 269\n",
      "Chunk text:\n",
      "    ## Arti fi cial Intelligence Index Report 2025\n",
      "    ![Image](hai_ai_index_report_2025_chapter_2-parsed-w-imgs_artifacts/image_000000\n",
      "    _c6ec2f8165962bd018633d07074eaf41cfb78512dd53037fa2fcdda5ff3e6b52.png)\n",
      "    ![Image](hai_ai_index_report_2025_chapter_2-parsed-w-imgs_artifacts/image_000001\n",
      "    _23acef5ec9d7e4fa2bc47b734cc808da8578bc8d91748aea27d76aedc4f31dd0.png)\n",
      "    ![Image](hai_ai_index_report_2025_chapter_2-parsed-w-imgs_artifacts/image_000002\n",
      "    _2ef37eac1d46d33cf738050ed266e05091775c5ea224676eb6a41aeb01cb00f1.pn...\n",
      "\n",
      "========================================\n",
      "Token count: 554\n",
      "Chunk text:\n",
      "     Chapter Highlights  85  ## 2.1 Overview of AI in 2024  87  Timeline: Signi fi\n",
      "    cant Model and Dataset Releases  87  State of AI Performance  93  Overall Review\n",
      "    93  Closed vs. Open-Weight Models  94  US vs. China Technical Performance  96\n",
      "    Improved Performance From Smaller Models  98  Model Performance Converges at the\n",
      "    Frontier  99  Benchmarking AI  100  ## 2.2 Language  ## 103  | Understanding\n",
      "    |   104 | |----------------------------------------------------...\n",
      "\n",
      "========================================\n",
      "Token count: 722\n",
      "Chunk text:\n",
      "     143  ![Image](hai_ai_index_report_2025_chapter_2-parsed-w-imgs_artifacts/image_\n",
      "    000003_4ad1321e55f50d6f7d0842da3bf083125f2096368e2d323ba074c40c9428ee29.png)\n",
      "    ![Image](hai_ai_index_report_2025_chapter_2-parsed-w-imgs_artifacts/image_000004\n",
      "    _4ad1321e55f50d6f7d0842da3bf083125f2096368e2d323ba074c40c9428ee29.png)  ##\n",
      "    Chapter 2: Technical Performance (cont'd)  | 2.8 AI Agents    |   144 |\n",
      "    |------------------|-------| | VisualAgentBench |   144 | | RE-Bench         |\n",
      "    145 | | GAIA             |   147 |...\n",
      "\n",
      "========================================\n",
      "Token count: 742\n",
      "Chunk text:\n",
      "    In  2023,  AI  researchers  introduced  several  challenging  new benchmarks,\n",
      "    including MMMU, GPQA, and SWE-bench, aimed at testing the limits of increasingly\n",
      "    capable AI systems. By 2024, AI performance on these benchmarks saw remarkable\n",
      "    improvements, with gains of 18.8 and 48.9 percentage points on MMMU and GPQA,\n",
      "    respectively. On SWE-bench, AI systems could solve just 4.4% of coding problems\n",
      "    in 2023-a fi gure that jumped to 71.7% in 2024. 2. Open-weight models catch up.\n",
      "    Last year's AI Index rev...\n",
      "\n",
      "========================================\n",
      "Token count: 594\n",
      "Chunk text:\n",
      "    The saturation of traditional AI benchmarks like MMLU, GSM8K, and HumanEval,\n",
      "    coupled with improved performance on newer, more challenging benchmarks such as\n",
      "    MMMU and GPQA, has pushed researchers to explore additional evaluation methods\n",
      "    for leading AI systems. Notable among these are Humanity's Last Exam, a rigorous\n",
      "    academic test where the top system scores just 8.80%; FrontierMath, a complex\n",
      "    mathematics benchmark where AI systems solve only 2% of problems; and\n",
      "    BigCodeBench, a coding benchmark wh...\n"
     ]
    }
   ],
   "source": [
    "for chunk in chunk_texts[:5]:\n",
    "    print(f\"\\n\" + \"=\" * 40)\n",
    "    print(f\"Token count: {chunk.token_count}\")\n",
    "    print(f\"Chunk text:\")\n",
    "    wrapped_text = textwrap.fill(chunk.text[:500]+\"...\", width=80)\n",
    "    print(textwrap.indent(wrapped_text, \"    \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a2a8a4",
   "metadata": {},
   "source": [
    "### Set up Weaviate Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "22933c0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "import utils\n",
    "\n",
    "# Helper function to connect to Weaviate\n",
    "client = utils.connect_to_weaviate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "842550b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.collections.delete(\"Chunks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4e202bae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<weaviate.collections.collection.sync.Collection at 0x1230000a0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from weaviate.classes.config import Property, DataType, Configure, Tokenization\n",
    "\n",
    "client.collections.create(\n",
    "    name=\"Chunks\",\n",
    "    properties=[\n",
    "        Property(\n",
    "            name=\"document_title\",\n",
    "            data_type=DataType.TEXT,\n",
    "        ),\n",
    "        Property(\n",
    "            name=\"chunk\",\n",
    "            data_type=DataType.TEXT,\n",
    "        ),\n",
    "        Property(\n",
    "            name=\"chunk_number\",\n",
    "            data_type=DataType.INT,\n",
    "        ),\n",
    "        Property(\n",
    "            name=\"filename\",\n",
    "            data_type=DataType.TEXT,\n",
    "            tokenization=Tokenization.FIELD\n",
    "        ),\n",
    "    ],\n",
    "    vector_config=[\n",
    "        Configure.Vectors.text2vec_cohere(\n",
    "            name=\"default\",\n",
    "            source_properties=[\"document_title\", \"chunk\"],\n",
    "            model=\"embed-v4.0\"\n",
    "        )\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7c9e2f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = client.collections.get(\"Chunks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f34f3bad",
   "metadata": {},
   "source": [
    "### Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dee53991",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "144it [00:00, 56116.30it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "with chunks.batch.fixed_size(batch_size=100) as batch:\n",
    "    for i, chunk_text in tqdm(enumerate(chunk_texts)):\n",
    "        obj = {\n",
    "            \"document_title\": \"Stanford HAI Report 2025\",\n",
    "            \"filename\": \"data/pdfs/hai_ai_index_report_2025_chapter_2.pdf\",\n",
    "            \"chunk\": chunk_text.text,\n",
    "            \"chunk_number\": i + 1,\n",
    "        }\n",
    "\n",
    "        # Add object to batch for import with (batch.add_object())\n",
    "        # BEGIN_SOLUTION\n",
    "        batch.add_object(\n",
    "            properties=obj\n",
    "        )\n",
    "        # END_SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ed8bc9",
   "metadata": {},
   "source": [
    "### RAG queries\n",
    "\n",
    "How do we perform RAG in this scenario? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd3e0fed",
   "metadata": {},
   "source": [
    "This is a bit different, because we haven't embedded the images (or stored them in Weaviate).\n",
    "\n",
    "In this scenario, let's:\n",
    "\n",
    "- Retrieve text chunks\n",
    "- Get images referred to in the text\n",
    "- Convert the images to base64\n",
    "- Send (retrieved text + images + prompt) to LLM for RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cae8dd66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================\n",
      "\n",
      "Self-driving vehicles have long been a goal for AI researchers and technologists. However, their widespread adoption has been  slower  than  anticipated.  Despite  many  predictions that  fully  autonomous  driving  is  imminent,  widespread  use of  self-driving  vehicles  has yet  to  become  a  reality.  Still,  in recent  years,  signi fi cant  progress  has  been  made.  In  cities like  San  Francisco  and  Phoenix, fl eets  of  self-driving  taxis are  now  operating  commercially.  This  section  examines recent advancements in autonomous driving, focusing on deployment, technological breakthroughs and new benchmarks, safety performance, and policy challenges.\n",
      "\n",
      "## Deployment\n",
      "\n",
      "Self-driving cars are increasingly being deployed worldwide. Cruise, a subsidiary of General Motors, launched its autonomous vehicles  in  San  Francisco  in  late  2022  before having its license suspended in 2023 after a litany of safety incidents. Waymo, a subsidiary of Alphabet, began deploying its  r...\n",
      "\n",
      "========================================\n",
      "\n",
      "China's  self-driving  revolution  is  also  accelerating,  led  by companies like Baidu's Apollo Go, which reported 988,000 rides across China in Q3 2024, re fl ecting a 20% year-over-year increase. In October 2024, the company was operating 400 robotaxis and announced plans to expand its fl eet to 1,000 by the end of 2025. Pony.AI, another Chinese autonomous vehicle manufacturer, has pledged to scale its robotaxi fl eet from 200 to at least 1,000 vehicles-with expectations that the fl eet will reach 2,000 to 3,000 by the end of 2026. China is leading the way in autonomous vehicle testing, with reports indicating  that  it  is  testing  more  driverless  cars  than  any other country and currently rolling them out across 16 cities. Robotaxis  in  China  are  notably  a ff ordable-even  cheaper, in  some  cases,  than  rides  provided  by  human  drivers.  To support  this growth, China  has prioritized establishing national regulations to govern the deployment of driverless cars.  Be...\n",
      "\n",
      "========================================\n",
      "\n",
      "Advancements  in  AI  over  the  past decade have paved the way for exciting new developments in the fi eld  of  robotics.  Especially  with  the rise of foundation models, robots are  now  able to  iteratively  learn  from their  surroundings,  adapt fl exibly  to new  settings,  and  make  autonomous decisions.  This  section  explores  key robotic benchmarks and recent trends, including the rise of humanoids, algorithmic advancements from DeepMind, and the emergence of robotic foundation models. It concludes  by  studying  developments in self-driving cars.\n",
      "\n",
      "## 2.9 Robotics and Autonomous Motion\n",
      "\n",
      "## Robotics\n",
      "\n",
      "## RLBench\n",
      "\n",
      "One of the most widely adopted benchmarks in the robotics community is RLBench (Robot Learning Benchmark). Launched in 2019, it features 100 unique tasks of varying complexity, from simple target reaching to opening an oven and placing a tray inside. 12 Researchers typically evaluate new robotic systems on a standardized subset of 18 tasks to gauge performance. Fig...\n",
      "\n",
      "========================================\n",
      "\n",
      "![Image](hai_ai_index_report_2025_chapter_2-parsed-w-imgs_artifacts/image_000205_b0bb4348efc1a7d526b1d1edb38b00657fdcab0ed1579ff817abfe336570f57c.png)\n",
      "\n",
      "2.9 Robotics and Automous Motion\n",
      "\n",
      "![Image](hai_ai_index_report_2025_chapter_2-parsed-w-imgs_artifacts/image_000206_2aefc4fbe24b88323284096026c8ed56e36caace03479e7cace884d7d0b7730b.png)\n",
      "\n",
      "## Safety Standards\n",
      "\n",
      "Emerging  research  suggests  that  self-driving  cars  may  be safer  than  human-driven  vehicles. ...\n",
      "\n",
      "========================================\n",
      "\n",
      "![Image](hai_ai_index_report_2025_chapter_2-parsed-w-imgs_artifacts/image_000198_a17133cd6d61bee855572dcecb0d996a199bcfc4ddf1ff00b1169fc76df76a7c.png)\n",
      "\n",
      "## Chapter 2: Technical Performance\n",
      "\n",
      "## Self-Driving Cars\n",
      "...\n",
      "\n",
      "========================================\n",
      "\n",
      "143\n",
      "\n",
      "![Image](hai_ai_index_report_2025_chapter_2-parsed-w-imgs_artifacts/image_000003_4ad1321e55f50d6f7d0842da3bf083125f2096368e2d323ba074c40c9428ee29.png)\n",
      "\n",
      "![Image](hai_ai_index_report_2025_chapter_2-parsed-w-imgs_artifacts/image_000004_4ad1321e55f50d6f7d0842da3bf083125f2096368e2d323ba074c40c9428ee29.png)\n",
      "\n",
      "## Chapter 2: Technical Performance (cont'd)\n",
      "\n",
      "| 2.8 AI Agents    |   144 |\n",
      "|------------------|-------|\n",
      "| VisualAgentBench |   144 |\n",
      "| RE-Bench         |   145 |\n",
      "| GAIA             |   147 |\n",
      "\n",
      "| 2.9 Robotics and Autonomous Motion        |   148 |\n",
      "|-------------------------------------------|-------|\n",
      "| Robotics                                  |   148 |\n",
      "| RLBench                                   |   148 |\n",
      "| Highlight: Humanoid Robotics              |   150 |\n",
      "| Highlight: DeepMind's Developments        |   151 |\n",
      "| Highlight: Foundation Models for Robotics |   154 |\n",
      "| Self-Driving Cars                         |   155 |\n",
      "| Deployment                                |   155 |\n",
      "| Technical ...\n",
      "\n",
      "========================================\n",
      "\n",
      "Figure 2.9.13\n",
      "\n",
      "Baidu's RT-6 Source: Verge, 2024\n",
      "\n",
      "![Image](hai_ai_index_report_2025_chapter_2-parsed-w-imgs_artifacts/image_000200_75b618b84a50065a5ffd19997404edddf41e30325da5200790a6e2bcefcd05cb.png)\n",
      "\n",
      "Figure 2.9.12\n",
      "\n",
      "![Image](hai_ai_index_report_2025_chapter_2-parsed-w-imgs_artifacts/image_000201_9b2b6205c26ef41aef4114c81d09d518035c02b18306a874d355a46289e449ce.png)\n",
      "\n",
      "Most existing benchmarks  for end-to-end autonomous driving rely on open-loop evaluation, which can be restrictive. Open-loop settings fail to test how autonomous agents  react  to  real-world  conditions  and  often  lead  to models that memorize driving patterns rather than learning to  drive  authentically.  While  closed-loop  benchmarks  like Town05Long and Longest6 exist, they primarily assess basic driving skills rather than performance in complex, interactive scenarios.  Bench2Drive  is  another  new  benchmark  that improves on these limitations by providing a comprehensive, realistic, closed-loop testing simulatio...\n",
      "\n",
      "========================================\n",
      "\n",
      "Waymo,  in  collaboration  with  Swiss  Re,  one  of  the  world's leading reinsurers, also conducted a study analyzing liability claims related to collisions over several million miles driven by its fully autonomous vehicles. The study compared Waymo's liability claims to human-driver baselines derived from Swiss Re's  extensive  dataset,  which  includes  over  500,000  claims and 200 billion miles of driving data. The results showed that Waymo vehicles had an 88% reduction in property damage claims  and  a  92%  reduction  in  bodily  injury  claims  (Figure 2.9.17). In real terms, across 25.3 million miles driven, Waymo vehicles were involved in just nine property damage claims and two bodily injury claims, whereas human drivers over the same distance  would  be  expected  to  incur  78  property  damage claims and 26 bodily injury claims. The Waymo drivers were also  signi fi cantly  safer  than  latest-generation  human-driven vehicles that are equipped with added safety feature...\n",
      "\n",
      "========================================\n",
      "\n",
      "## DeepMind's Developments (cont'd)\n",
      "\n",
      "which  enables  robots  to  interpret  3D  environments,  and  in image processing, SARA-based models run signi fi cantly faster while avoiding major increases in run-time at scale.\n",
      "\n",
      "Other developments from DeepMind include ALOHA (Autonomous Learning of High-level Activities) and DemoStart. ALOHA Unleashed is a breakthrough in enabling robots  to  perform  intricate  dexterous  manipulation  tasks, such as tying shoelaces or hanging T-shirts on coat hangers- tasks  that  historically  have  been  extremely  challenging  for robots. The researchers demonstrated that combining a large imitation learning dataset with a transformer-based learning architecture  is  a  highly  e ff ective  approach  for  overcoming these  di ffi culties.  The  ALOHA  approach  enabled  Google's robot to e ff ectively learn a diverse range of tasks, including hanging a shirt, stacking kitchen items, and tying shoelaces (Figure 2.9.7). As shown in Figure 2.9.8, ALOHA-train...\n",
      "\n",
      "========================================\n",
      "\n",
      "## Highlight:\n",
      "\n",
      "## DeepMind's Developments (cont'd)\n",
      "\n",
      "Similarly,  DemoStart  introduces  a  novel  auto-curriculum reinforcement learning method that enables a robotic arm to  master  complex  behaviors  using  only  sparse  rewards and a limited number of demonstrations. This breakthrough highlights the potential for robots to learn e ffi ciently with minimal data, reducing the need for data-intensive training and making advanced robotics more accessible and widely\n",
      "\n",
      "![Image](hai_ai_index_report_2025_chapter_2-parsed-w-imgs_artifacts/image_000195_75b618b84a50065a5ffd19997404edddf41e30325da5200790a6e2bcefcd05cb.png)\n",
      "\n",
      "adopted.  DeepMind  also  introduced  a  robotic  model  in 2024 that was capable of reaching amateur human-level performance  in  competitive  table  tennis  (Figure  2.9.9). Given that achieving human-level speed and performance on real-world tasks is an important benchmark for robotics research,  this  achievement  is  a  notable  step  forward  in robotic ability.\n",
      "\n",
      "## Ro...\n"
     ]
    }
   ],
   "source": [
    "response = chunks.query.hybrid(\n",
    "    query=\"Latest developments in self-driving cars / autonomous vehicles\",\n",
    "    limit=10\n",
    ")\n",
    "\n",
    "for o in response.objects:\n",
    "    print(f\"\\n\" + \"=\" * 40)\n",
    "    print(o.properties[\"chunk\"][:1000] + \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5a729583",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_image_paths(text):\n",
    "    \"\"\"Extract image paths from markdown-style image references.\"\"\"\n",
    "    pattern = r'!\\[.*?\\]\\((.*?)\\)'\n",
    "    return re.findall(pattern, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9d036efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_base64s(image_paths, base_path=None):\n",
    "    import base64\n",
    "    base64_images = []\n",
    "    for img_path in image_paths:\n",
    "        full_path = Path(base_path) / img_path if base_path else Path(img_path)\n",
    "        image_bytes = full_path.read_bytes()\n",
    "        base64_string = base64.b64encode(image_bytes).decode(\"utf-8\")\n",
    "        base64_images.append(base64_string)\n",
    "\n",
    "    return base64_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "430397bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_chunks = \"\"\n",
    "all_images = []\n",
    "\n",
    "for o in response.objects:\n",
    "    chunk_text = o.properties[\"chunk\"]\n",
    "    image_paths = extract_image_paths(chunk_text)\n",
    "    all_images.extend(get_image_base64s(image_paths, base_path=\"data/parsed\"))\n",
    "\n",
    "    all_chunks += \"\\n\\n\" + chunk_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d93da885",
   "metadata": {},
   "outputs": [],
   "source": [
    "task_text = \"\"\"\n",
    "What developments in self-driving cars / autonomous vehicles are mentioned here? Answer based on the provided text and images.\n",
    "\n",
    "Describe the details from the figures as well, if necessary.\n",
    "\"\"\" + \"\\n\\n\" + all_chunks\n",
    "\n",
    "message = {\n",
    "    \"role\": \"user\", \"content\": [\n",
    "        {\"type\": \"text\", \"text\": task_text}\n",
    "    ]\n",
    "}\n",
    "\n",
    "for img in all_images:\n",
    "    content = {\n",
    "        \"type\": \"image\",\n",
    "        \"source\": {\n",
    "            \"type\": \"base64\",\n",
    "            \"media_type\": \"image/png\",\n",
    "            \"data\": img,\n",
    "        }\n",
    "    }\n",
    "    # Append `content`` to message[\"content\"]\n",
    "    # BEGIN_SOLUTION\n",
    "    message[\"content\"].append(content)\n",
    "    # END_SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee4c148",
   "metadata": {},
   "outputs": [],
   "source": [
    "import anthropic\n",
    "\n",
    "anthropic_response = anthropic.Anthropic().messages.create(\n",
    "    model=\"claude-3-5-haiku-latest\",\n",
    "    max_tokens=1024,\n",
    "    # Add [message] as the messages to pass to Claude\n",
    "    # BEGIN_SOLUTION\n",
    "    messages=[message]\n",
    "    # END_SOLUTION\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "754f2923",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided text and images, here are the key developments in self-driving cars/autonomous vehicles mentioned:\n",
      "\n",
      "## Deployment Developments\n",
      "\n",
      "**United States:**\n",
      "- **Waymo** (Alphabet subsidiary) has emerged as a leading player, operating in four major U.S. cities: Phoenix, San Francisco, Los Angeles, and Austin\n",
      "- As of January 2025, Waymo provides 150,000 paid rides per week, covering over 1 million miles\n",
      "- Plans to test in 10 additional cities including Las Vegas, San Diego, Miami, upstate New York, and Truckee, California (specifically chosen for snowy weather testing)\n",
      "\n",
      "- **Cruise** (General Motors subsidiary) launched in San Francisco in late 2022 but had its license suspended in 2023 due to safety incidents\n",
      "\n",
      "- **Self-driving trucks**: Companies like Kodiak completed first driverless deliveries, and Aurora reported over 1 million miles of autonomous freight hauling on U.S. highways since 2021 (with human safety drivers present). Aurora delayed its commercial launch from end of 2024 to April 2025.\n",
      "\n",
      "**China:**\n",
      "- **Baidu's Apollo Go** reported 988,000 rides across China in Q3 2024 (20% year-over-year increase)\n",
      "- Operating 400 robotaxis with plans to expand to 1,000 by end of 2025\n",
      "- **Pony.AI** pledged to scale from 200 to at least 1,000 vehicles, expecting 2,000-3,000 by end of 2026\n",
      "- China is testing more driverless cars than any other country across 16 cities\n",
      "- Robotaxis in China are notably affordable, sometimes cheaper than human drivers\n",
      "- China has prioritized establishing national regulations for driverless cars\n",
      "\n",
      "## Technical Innovations and New Vehicle Developments\n",
      "\n",
      "**New Vehicle Launches:**\n",
      "- **Tesla Cybercab**: Two-passenger autonomous vehicle without steering wheel or pedals, set for 2026 production at under $30,000\n",
      "- **Tesla Robovan**: Electric autonomous van for up to 20 passengers\n",
      "- **Baidu RT6**: Latest-generation robotaxi priced at $30,000 with battery-swapping system, launched across multiple Chinese cities\n",
      "\n",
      "**New Benchmarks for Testing:**\n",
      "- **nuPlan** (by Motional): Large-scale autonomous driving dataset with 1,282 hours of diverse driving scenarios from multiple cities\n",
      "- **OpenAD**: First real-world, open-world autonomous driving benchmark for 3D object detection, focusing on domain generalization\n",
      "- **Bench2Drive**: Comprehensive closed-loop testing simulation with over 2 million annotated frames and 220 short routes for testing in diverse conditions\n",
      "\n",
      "## Safety Performance\n",
      "\n",
      "**Waymo Safety Study:**\n",
      "- Collaboration with Swiss Re analyzed liability claims over several million miles\n",
      "- Results showed 88% reduction in property damage claims and 92% reduction in bodily injury claims compared to human drivers\n",
      "- Across 25.3 million miles: only 9 property damage claims and 2 bodily injury claims for Waymo vs. expected 78 property damage and 26 bodily injury claims for human drivers\n",
      "\n",
      "The figures show the growth in Waymo's rider-only miles driven without human drivers, the RT6 vehicle from Baidu, driving score comparisons on the Bench2Drive benchmark, and the dramatic reduction in insurance claims for Waymo vehicles compared to human-driven vehicles.\n"
     ]
    }
   ],
   "source": [
    "print(anthropic_response.content[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "54d3533e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "{\"build_git_commit\":\"08d409a988\",\"build_go_version\":\"go1.25.0\",\"build_image_tag\":\"HEAD\",\"build_wv_version\":\"1.32.5\",\"error\":\"cannot find peer\",\"level\":\"error\",\"msg\":\"transferring leadership\",\"time\":\"2025-09-16T12:04:03+01:00\"}\n"
     ]
    }
   ],
   "source": [
    "client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d67fd35",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "workshop-pdf-rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
